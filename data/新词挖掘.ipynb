{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 新词挖掘"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T02:28:23.426088Z",
     "start_time": "2021-09-24T02:28:23.418927Z"
    }
   },
   "source": [
    "用于挖掘特定领域场景的新词，这里提供两种方法，分别是**基于频次**和**基于自由凝固度以及左右邻字熵**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基于频次的新词挖掘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T09:21:23.516012Z",
     "start_time": "2021-09-24T09:21:20.971619Z"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from ltp import LTP\n",
    "\n",
    "ltp = LTP()\n",
    "with open(\"./test.txt\", \"r\", encoding = \"utf-8\") as f:\n",
    "    texts = f.readlines()[:100000]  # 因语料太大，所以这里只用了前1W条做新词发现\n",
    "    with tqdm(range(0, len(texts), 1000)) as pbar:\n",
    "        words = []\n",
    "        for i in pbar:\n",
    "            words.extend([word for text in ltp.seg(texts[i:i+1000])[0] for word in text])\n",
    "print(len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T09:29:05.179009Z",
     "start_time": "2021-09-24T09:29:03.411788Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from ltp import LTP\n",
    "ltp = LTP()\n",
    "\n",
    "text = '''\n",
    "自然语言处理( Natural Language Processing, NLP)是计算机科学领域与人工智能领域中的一个重要方向。它研究能实现人与计算机之间用自然语言进行有效通信的各种理论和方法。自然语言处理是一门融语言学、计算机科学、数学于一体的科学。因此，这一领域的研究将涉及自然语言，即人们日常使用的语言，所以它与语言学的研究有着密切的联系，但又有重要的区别。自然语言处理并不是一般地研究自然语言，而在于研制能有效地实现自然语言通信的计算机系统，特别是其中的软件系统。因而它是计算机科学的一部分。\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T09:29:15.099171Z",
     "start_time": "2021-09-24T09:29:15.048454Z"
    }
   },
   "outputs": [],
   "source": [
    "print(ltp.seg([text])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T09:32:28.634031Z",
     "start_time": "2021-09-24T09:32:28.327963Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_chinese_words(file_path):\n",
    "    with open(file_path, \"r\", encoding = \"utf-8\") as f:\n",
    "        return [line.split()[0] for line in f.readlines()]\n",
    "\n",
    "CH_DICT = set(get_chinese_words(\"chinese_words.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T09:32:32.021425Z",
     "start_time": "2021-09-24T09:32:31.989504Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "unigram_freq, bigram_freq = {},{}\n",
    "for i in range(len(words)-1):\n",
    "    if words[i] not in CH_DICT and not re.search(\"[^\\u4e00-\\u9fa5]\",words[i]): \n",
    "        if words[i] in unigram_freq: # 一阶计数\n",
    "            unigram_freq[words[i]] += 1\n",
    "        else:\n",
    "            unigram_freq[words[i]] = 1\n",
    "    bigram = words[i]+words[i+1]\n",
    "    if bigram not in CH_DICT and not re.search(\"[^\\u4e00-\\u9fa5]\",bigram): \n",
    "        if bigram in bigram_freq:\n",
    "            bigram_freq[bigram] += 1\n",
    "        else:\n",
    "            bigram_freq[bigram] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T09:32:36.118237Z",
     "start_time": "2021-09-24T09:32:36.107604Z"
    }
   },
   "outputs": [],
   "source": [
    "unigram_freq_sorted = sorted(unigram_freq.items(), key = lambda d: d[1],reverse = True)\n",
    "bigram_freq_sorted = sorted(bigram_freq.items(), key = lambda d: d[1],reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T09:39:54.276828Z",
     "start_time": "2021-09-24T09:39:54.266340Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"unigram:\\n\", unigram_freq_sorted[:100])\n",
    "print(\"\\n\")\n",
    "print(\"bigram:\\n\", bigram_freq_sorted[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基于自由疑固度以及左右邻字熵的新词挖掘"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 自由疑固度：表示一个字串的凝固程度。\n",
    "<center>$$pmi(x,y) = log{\\frac{P(x,y)}{P(x)P(y)}}$$</center>、\n",
    "- 左邻字熵与右邻字熵：表示一个字串左右搭配的丰富性。\n",
    "<center>$$entropy(w) = -P(x_i)logP(x_i)$$</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**实战流程**  \n",
    "第一步：数据获取及预处理；词典获取；  \n",
    "第二步：将数据进行切分获取所有切分出的候选单词，并且统计词频信息、候选新词左右出现的字的信息；  \n",
    "第三步：根据第二步中统计的进行 pmi 值以及左右邻字熵的计算；  \n",
    "第四步：设定各指标的阈值，根据其值获取新词结果；  \n",
    "第五步：根据一些规则过滤掉明显不正确的新词，得到最终的新词结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据获取及预处理；词典获取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-27T00:19:49.179030Z",
     "start_time": "2021-09-27T00:19:43.722062Z"
    }
   },
   "outputs": [],
   "source": [
    "# 读取数据\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from ltp import LTP\n",
    "ltp = LTP()\n",
    "    \n",
    "def preprocess_data(file_path):\n",
    "    texts = []\n",
    "    with open(file_path, \"r\", encoding = \"utf-8\") as f:\n",
    "        lines = f.readlines()[:100000]\n",
    "        with tqdm(lines, total=len(lines)) as pbar:\n",
    "            for text in pbar:\n",
    "                text = re.sub(\"[^\\u4e00-\\u9fa5。？．，！：]\", \"\", text.strip()) \n",
    "    #             text_splited = re.split(\"[。？．，！：]\", text) \n",
    "                text_splited = ltp.sent_split([text])  # 调用LTP进行分句\n",
    "                texts.extend(text_splited)\n",
    "            \n",
    "    tmp = texts\n",
    "    texts = []\n",
    "    with tqdm(tmp, total=len(tmp), desc=\"filtering the null sentences\") as pbar:\n",
    "        for text in pbar:\n",
    "            if text is not \"\":\n",
    "                texts.append(text)\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-27T00:19:51.054557Z",
     "start_time": "2021-09-27T00:19:51.015435Z"
    }
   },
   "outputs": [],
   "source": [
    "texts = preprocess_data(\"test.txt\")  # 按照基本的标点符号进行切分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-27T00:19:53.604507Z",
     "start_time": "2021-09-27T00:19:53.340778Z"
    }
   },
   "outputs": [],
   "source": [
    "# 获取已有的中文词典\n",
    "def get_chinese_words(file_path):\n",
    "    with open(file_path, \"r\", encoding = \"utf-8\") as f:\n",
    "        return [line.split()[0] for line in f.readlines()]\n",
    "\n",
    "CH_DICT = set(get_chinese_words(\"chinese_words.txt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 将数据进行切分获取所有切分出的候选单词，并且统计词频信息、候选新词左右出现的字的信息"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来需要对文本进行切分以及获取相关的频次信息，这里统一在一个函数中，主要逻辑如下：\n",
    "- 对文本按照一定的长度范围进行切分，切分出所有成词的可能性，这里称之为字符串；\n",
    "- 对于所有切分出的字符串进行过滤，长度大于等于 2 的词以及不是词典 CH_DICT 中的词作为候选新词；\n",
    "- 获取所有切分出的字符串的频次信息（在后续计算中需要用到一些字符串的频次信息）、候选新词词频信息、候选新词左右出现的字的统计信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-27T00:19:53.913456Z",
     "start_time": "2021-09-27T00:19:53.874699Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_candidate_wordsinfo(texts, max_word_len):\n",
    "    '''\n",
    "    texts：表示输入的所有文本\n",
    "    max_word_len：表示最长的词长    \n",
    "    '''\n",
    "    # 四个词典均以单词为 key，分别以词频、候选新词词频、左字集合、右字集合为 value\n",
    "    words_freq, candidate_words_freq, candidate_words_left_characters, candidate_words_right_characters = {},{},{},{}\n",
    "    WORD_NUM = 0  # 统计所有可能的字符串频次\n",
    "    with tqdm(texts, total=len(texts)) as pbar:\n",
    "        for text in pbar:  # 遍历每个文本\n",
    "            # word_indexes 中存储了所有可能的词汇的切分下标 (i,j) ，i 表示词汇的起始下标，j 表示结束下标，注意这里有包括了所有的字\n",
    "            # word_indexes 的生成需要两层循环，第一层循环，遍历所有可能的起始下标 i；第二层循环，在给定 i 的情况下，遍历所有可能的结束下标 j\n",
    "            word_indexes = [(i,j) for i in range(len(text)) for j in range(i + 1, min(i + 1 + max_word_len, len(text)+1))]\n",
    "            WORD_NUM += len(word_indexes)\n",
    "            for index in word_indexes:  # 遍历所有词汇的下标\n",
    "                word = text[index[0]:index[1]]  # 获取单词\n",
    "                # 更新所有切分出的字符串的频次信息\n",
    "                if word in words_freq:\n",
    "                    words_freq[word] += 1\n",
    "                else:\n",
    "                    words_freq[word] = 1\n",
    "                if len(word) >= 2 and word not in CH_DICT:  # 长度大于等于 2 的词以及不是词典中的词作为候选新词\n",
    "                    # 更新候选新词词频\n",
    "                    if word in candidate_words_freq:\n",
    "                        candidate_words_freq[word] += 1\n",
    "                    else:\n",
    "                        candidate_words_freq[word] = 1\n",
    "                    # 更新候选新词左字集合\n",
    "                    if index[0] != 0:  # 当为文本中首个单词时无左字\n",
    "                        if word in candidate_words_left_characters:\n",
    "                            candidate_words_left_characters[word].append(text[index[0]-1])\n",
    "                        else:\n",
    "                            candidate_words_left_characters[word] = [text[index[0]-1]]\n",
    "                    else:\n",
    "                        if word in candidate_words_left_characters:\n",
    "                            candidate_words_left_characters[word].append(len(candidate_words_left_characters[word]))\n",
    "                        else:\n",
    "                            candidate_words_left_characters[word] = [0]                    \n",
    "                    # 更新候选新词右字集合\n",
    "                    if index[1] < len(text)-1:  # 当为文本中末个单词时无右字\n",
    "                        if word in candidate_words_right_characters:\n",
    "                            candidate_words_right_characters[word].append(text[index[1]]) # \n",
    "                        else:\n",
    "                            candidate_words_right_characters[word] = [text[index[1]]]\n",
    "                    else:\n",
    "                        if word in candidate_words_right_characters:\n",
    "                            candidate_words_right_characters[word].append(len(candidate_words_right_characters[word]))\n",
    "                        else:\n",
    "                            candidate_words_right_characters[word] = [0]\n",
    "    return WORD_NUM, words_freq, candidate_words_freq, candidate_words_left_characters, candidate_words_right_characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-27T00:19:59.491903Z",
     "start_time": "2021-09-27T00:19:59.163367Z"
    }
   },
   "outputs": [],
   "source": [
    "WORD_NUM, words_freq, candidate_words_freq, candidate_words_left_characters, candidate_words_right_characters = \\\n",
    "get_candidate_wordsinfo(texts = texts, max_word_len = 6)  # 字符串最长为 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 根据第二步中统计的进行 pmi 值以及左右邻字熵的计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-27T00:20:00.492563Z",
     "start_time": "2021-09-27T00:20:00.479023Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# 计算候选单词的 pmi 值\n",
    "def compute_pmi(words_freq, candidate_words_freq):\n",
    "    words_pmi = {}\n",
    "    with tqdm(candidate_words_freq, total=len(candidate_words_freq), desc=\"Counting pmi\") as pbar:\n",
    "        for word in pbar:\n",
    "            # 首先，将某个候选单词按照不同的切分位置切分成两项，比如“电影院”可切分为“电”和“影院”以及“电影”和“院”\n",
    "            bi_grams = [(word[0:i],word[i:]) for i in range(1,len(word))]\n",
    "            # 对所有切分情况计算 pmi 值，取最大值作为当前候选词的最终 pmi 值\n",
    "            # words_freq[bi_gram[0]]，words_freq[bi_gram[1]] 分别表示一个候选新词的前后两部分的出现频次\n",
    "            words_pmi[word] = max(map(lambda bi_gram: math.log(\\\n",
    "                words_freq[word]/(words_freq[bi_gram[0]]*words_freq[bi_gram[1]]/WORD_NUM)), bi_grams))\n",
    "    return words_pmi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在下一步中，计算 pmi 值以及左右邻字熵。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-27T00:20:01.586222Z",
     "start_time": "2021-09-27T00:20:01.321617Z"
    }
   },
   "outputs": [],
   "source": [
    "words_pmi = compute_pmi(words_freq, candidate_words_freq)\n",
    "words_pmi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-27T00:20:01.953164Z",
     "start_time": "2021-09-27T00:20:01.946786Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter \n",
    "\n",
    "# 计算候选单词的邻字熵\n",
    "def compute_entropy(candidate_words_characters):\n",
    "    words_entropy = {}\n",
    "    with tqdm(candidate_words_characters.items(), total=len(candidate_words_characters), desc=\"Counting entropy\") as pbar:\n",
    "        for word, characters in pbar:\n",
    "            character_freq = Counter(characters)  # 统计邻字的出现分布\n",
    "            # 根据出现分布计算邻字熵\n",
    "            words_entropy[word] = sum(map(lambda x: - x/len(characters) * math.log(x/len(characters)) , character_freq.values())) \n",
    "    return words_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-27T00:20:02.894920Z",
     "start_time": "2021-09-27T00:20:02.321323Z"
    }
   },
   "outputs": [],
   "source": [
    "words_left_entropy = compute_entropy(candidate_words_left_characters)\n",
    "words_right_entropy = compute_entropy(candidate_words_right_characters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 设定各指标的阈值，根据其值获取最终的新词结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-27T00:20:03.290762Z",
     "start_time": "2021-09-27T00:20:03.284101Z"
    }
   },
   "outputs": [],
   "source": [
    "# 根据各指标阈值获取最终的新词结果\n",
    "def get_newwords(candidate_words_freq,\n",
    "                               words_pmi,\n",
    "                               words_left_entropy,\n",
    "                               words_right_entropy,\n",
    "                               words_freq_limit=4,\n",
    "                               pmi_limit=5,\n",
    "                               entropy_limit=1):\n",
    "    # 在每一项指标中根据阈值进行筛选\n",
    "    candidate_words = [k for k, v in candidate_words_freq.items() if v >= words_freq_limit]\n",
    "    candidate_words_pmi = [k for k, v in words_pmi.items() if v >= pmi_limit]\n",
    "    candidate_words_left_entropy = [k for k, v in words_left_entropy.items() if v >= entropy_limit]\n",
    "    candidate_words_right_entropy = [k for k, v in words_right_entropy.items() if v >= entropy_limit]\n",
    "    # 对筛选结果进行合并\n",
    "    return list(set(candidate_words).intersection(candidate_words_pmi, candidate_words_left_entropy, candidate_words_right_entropy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-27T00:20:31.263048Z",
     "start_time": "2021-09-27T00:20:31.225118Z"
    }
   },
   "outputs": [],
   "source": [
    "# 可以不断调参数来达到想要的结果\n",
    "new_words = get_newwords(candidate_words_freq,\n",
    "                         words_pmi,\n",
    "                         words_left_entropy,\n",
    "                         words_right_entropy,\n",
    "                         words_freq_limit= 2,\n",
    "                         pmi_limit=3,\n",
    "                         entropy_limit=1)\n",
    "print(len(new_words))\n",
    "new_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 过滤掉一些不正确的新词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-27T00:20:37.356733Z",
     "start_time": "2021-09-27T00:20:37.340186Z"
    }
   },
   "outputs": [],
   "source": [
    "new_words1 = list(filter(lambda x: not re.search(\"[^\\u4e00-\\u9fa5]\", x), new_words))\n",
    "new_words2 = list(filter(lambda x: not re.search(\"[了但里的和为是]\", x), new_words1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-27T00:20:47.240200Z",
     "start_time": "2021-09-27T00:20:47.226105Z"
    }
   },
   "outputs": [],
   "source": [
    "print(len(new_words2))\n",
    "new_words2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-27T00:20:38.513825Z",
     "start_time": "2021-09-27T00:20:38.463910Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(\"new_words.txt\", \"\", encoding=\"utf-8\") as f:\n",
    "    for new_word in new_words2:\n",
    "        f.write(new_word + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch1.7",
   "language": "python",
   "name": "torch1.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "321.733px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
